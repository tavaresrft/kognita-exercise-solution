{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = \"resources/dataset_2021-5-26-10-14.csv\" # Caminho para o arquivo onde há o data_set\n",
    "test_size = 0.2 # Tamanho percentual do dataset de teste\n",
    "# Lista com as colunas que serão usadas no dataset\n",
    "feature_columns = [\"default_3months\",\"dividas_vencidas_qtd\", \"dividas_vencidas_valor\", \"acao_judicial_valor\",\"month\"]\n",
    "saved_model_dir = \"saved_model\" # Pasta onde serão salvos o modelo e o scaler\n",
    "saved_model_file = \"classifier.joblib\" # arquivo onde será salvo o modelo\n",
    "saved_transform_file = \"scaler.joblib\" # arquivo onde será salvo o scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta o dataset para o modelo a partir do dataset dado\n",
    "# de forma que a quantidade de labels sejam iguais:\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(dataset_file_path, sep=\"\\t\", encoding=\"utf-8\")\n",
    "default_1 = dataset[dataset[\"default\"]==1]\n",
    "default_0 = dataset[dataset[\"default\"]==0].sample(n=default_1[\"default\"].count())\n",
    "model_dataset = pd.concat([default_0,default_1]).sample(n=2*default_1[\"default\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa o dataset em features e classes\n",
    "label = model_dataset[\"default\"]\n",
    "features_dataset = model_dataset.filter(feature_columns)\n",
    "\n",
    "#  Separa as features e as classes em datasets de teste e treino\n",
    "features_train, features_test, label_train, label_test = train_test_split(features_dataset.values, label.values, \n",
    "                                                    shuffle = True, \n",
    "                                                    test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/scaler.joblib']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reescala os dados com o standard scaler e salva a transformação\n",
    "# para transformações futuras\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_test = scaler.transform(features_test)\n",
    "dump(scaler, f\"{saved_model_dir}/{saved_transform_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/scaler.joblib']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treina o modelo usando regressão logística e salva para predições futuras\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(features_train, label_train)\n",
    "dump(classifier, f\"{saved_model_dir}/{saved_transform_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2906 1027]\n",
      " [1881 2004]]\n"
     ]
    }
   ],
   "source": [
    "# Faz as predições e calcula a matriz de confusão\n",
    "\n",
    "y_pred = classifier.predict(features_test)\n",
    "cm = confusion_matrix(label_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.6249680225121514\n",
      "precision =  0.7334170854271357\n",
      "recall =  0.6093945720250522\n"
     ]
    }
   ],
   "source": [
    "# Calcula as métricas: accuracy, precision, recall\n",
    "\n",
    "true_positive = cm[0][0]\n",
    "true_negative = cm[1][1]\n",
    "false_positive = cm[0][1]\n",
    "false_negative = cm[1][0]\n",
    "\n",
    "print(\"accuracy = \",(true_positive+true_negative)/(true_positive+true_negative+false_negative+false_positive))\n",
    "print(\"precision = \",(true_positive)/(true_positive+false_positive))\n",
    "print(\"recall = \",(true_positive)/(true_positive+false_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
